{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRLySbbo4_Jx",
        "outputId": "dcfd498d-82d5-40d7-96bd-031d31321461"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# MEMORY-EFFICIENT BALANCED DATASET CREATION (WITH PROGRESS LOGS)\n",
        "# Retina = 70k Train + 15k Val\n",
        "# Non-Retina = 30k Train + 15k Val  (CIFAR + COCO ONLY)\n",
        "# FULL SHUFFLING ENABLED FOR MAXIMUM DIVERSITY\n",
        "# ================================================================\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import shutil\n",
        "import random\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "from zipfile import ZipFile\n",
        "\n",
        "print(\"\\nüöÄ STARTING BALANCED DATASET GENERATION PIPELINE...\\n\")\n",
        "\n",
        "# ================================================================\n",
        "# 1) DOWNLOAD RETINAL DATASET USING KAGGLEHUB\n",
        "# ================================================================\n",
        "print(\"üìå STEP 1: Installing KaggleHub & Downloading Retinal Dataset\")\n",
        "!pip install -q kagglehub\n",
        "import kagglehub\n",
        "\n",
        "print(\"üì• Downloading retinal dataset via KaggleHub...\")\n",
        "kaggle_path = kagglehub.dataset_download(\n",
        "    \"ascanipek/eyepacs-aptos-messidor-diabetic-retinopathy\"\n",
        ")\n",
        "print(\"‚úî Retina dataset downloaded at:\", kaggle_path)\n",
        "\n",
        "# ================================================================\n",
        "# TARGET SIZES\n",
        "# ================================================================\n",
        "RETINA_TRAIN_TARGET = 20000\n",
        "RETINA_VAL_TARGET   = 8000\n",
        "\n",
        "NONRET_TRAIN_TARGET = 20000\n",
        "NONRET_VAL_TARGET   = 8000\n",
        "\n",
        "# ================================================================\n",
        "# 2) COLLECT RETINAL IMAGES (WITH SHUFFLING)\n",
        "# ================================================================\n",
        "print(\"\\nüìå STEP 2: Collecting retinal images...\")\n",
        "\n",
        "retinal_images = []\n",
        "for root, dirs, files in os.walk(kaggle_path):\n",
        "    for f in files:\n",
        "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\")):\n",
        "            retinal_images.append(os.path.join(root, f))\n",
        "\n",
        "print(f\"üîç Found {len(retinal_images)} raw retinal images.\")\n",
        "\n",
        "# üî• SHUFFLE FOR FULL DIVERSITY\n",
        "random.shuffle(retinal_images)\n",
        "\n",
        "required_retina = RETINA_TRAIN_TARGET + RETINA_VAL_TARGET\n",
        "retina_train = retinal_images[:RETINA_TRAIN_TARGET]\n",
        "retina_val   = retinal_images[RETINA_TRAIN_TARGET:required_retina]\n",
        "\n",
        "# üî• SHUFFLE TRAIN AND VAL SEPARATELY\n",
        "random.shuffle(retina_train)\n",
        "random.shuffle(retina_val)\n",
        "\n",
        "print(f\"üìä Retinal Train Count: {len(retina_train)}\")\n",
        "print(f\"üìä Retinal Val Count:   {len(retina_val)}\")\n",
        "\n",
        "# ================================================================\n",
        "# 3) CREATE FINAL FOLDER STRUCTURE\n",
        "# ================================================================\n",
        "print(\"\\nüìå STEP 3: Creating final dataset folder structure...\")\n",
        "\n",
        "base_dir = \"/content/retina_nonretina_dataset\"\n",
        "\n",
        "paths = {\n",
        "    \"train_retinal\": f\"{base_dir}/train/retinal\",\n",
        "    \"train_nonret\":  f\"{base_dir}/train/non-retinal\",\n",
        "    \"val_retinal\":   f\"{base_dir}/val/retinal\",\n",
        "    \"val_nonret\":    f\"{base_dir}/val/non-retinal\",\n",
        "}\n",
        "\n",
        "for p in paths.values():\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "\n",
        "print(\"‚úî Final folders created!\")\n",
        "\n",
        "# ================================================================\n",
        "# COPY RETINAL IMAGES\n",
        "# ================================================================\n",
        "print(\"\\nüìå STEP 4: Copying retinal TRAIN images...\")\n",
        "for img in tqdm(retina_train, desc=\"Copying Retinal Train\"):\n",
        "    shutil.copy(img, paths[\"train_retinal\"])\n",
        "\n",
        "print(\"\\nüìå Copying retinal VAL images...\")\n",
        "for img in tqdm(retina_val, desc=\"Copying Retinal Val\"):\n",
        "    shutil.copy(img, paths[\"val_retinal\"])\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# 4) NON-RETINAL IMAGES (CIFAR + COCO ONLY, FULL SHUFFLING)\n",
        "# ================================================================\n",
        "print(\"\\nüìå STEP 5: Collecting Non-Retinal Images from CIFAR & COCO...\")\n",
        "\n",
        "nonret_raw = \"/content/nonret_raw\"\n",
        "os.makedirs(nonret_raw, exist_ok=True)\n",
        "all_nonret = []\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# üìå SOURCE 1: CIFAR-10 & CIFAR-100\n",
        "# -----------------------------------------------------------\n",
        "print(\"\\nüì• Loading CIFAR-10 and CIFAR-100...\")\n",
        "from tensorflow.keras.datasets import cifar10, cifar100\n",
        "\n",
        "(X10, _), _ = cifar10.load_data()\n",
        "(X100, _), _ = cifar100.load_data()\n",
        "\n",
        "# üî• SHUFFLE CIFAR BEFORE SAVING\n",
        "np.random.shuffle(X10)\n",
        "np.random.shuffle(X100)\n",
        "\n",
        "def save_cifar(images, folder):\n",
        "    out = f\"{nonret_raw}/{folder}\"\n",
        "    os.makedirs(out, exist_ok=True)\n",
        "    print(f\"üñº Saving {folder} images...\")\n",
        "    for i, img in enumerate(images):\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "        path = f\"{out}/{folder}_{i}.jpg\"\n",
        "        cv2.imwrite(path, img)\n",
        "        all_nonret.append(path)\n",
        "\n",
        "save_cifar(X10, \"cifar10\")      # 50,000 images\n",
        "save_cifar(X100, \"cifar100\")    # 50,000 images\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# üìå SOURCE 2: COCO 2017 train subset\n",
        "# -----------------------------------------------------------\n",
        "print(\"\\nüì• Downloading COCO 2017 train subset (~40,000 images)...\")\n",
        "\n",
        "coco_zip = \"/content/train2017.zip\"\n",
        "coco_url = \"http://images.cocodataset.org/zips/train2017.zip\"\n",
        "\n",
        "with requests.get(coco_url, stream=True) as r:\n",
        "    total = int(r.headers.get(\"content-length\", 0))\n",
        "    with open(coco_zip, \"wb\") as f, tqdm(total=total, desc=\"Downloading COCO\", unit=\"B\", unit_scale=True):\n",
        "        for chunk in r.iter_content(chunk_size=1024 * 1024):\n",
        "            f.write(chunk)\n",
        "\n",
        "coco_out = f\"{nonret_raw}/coco\"\n",
        "os.makedirs(coco_out, exist_ok=True)\n",
        "\n",
        "with ZipFile(coco_zip, \"r\") as z:\n",
        "    jpgs = [x for x in z.namelist() if x.endswith(\".jpg\")]\n",
        "\n",
        "    # üî• SHUFFLE COCO LIST BEFORE SAMPLING\n",
        "    random.shuffle(jpgs)\n",
        "\n",
        "    print(\"üìÇ Extracting 40,000 COCO images...\")\n",
        "    for f in tqdm(jpgs[:40000], desc=\"Extracting COCO\"):\n",
        "        z.extract(f, coco_out)\n",
        "        all_nonret.append(os.path.join(coco_out, f))\n",
        "\n",
        "os.remove(coco_zip)\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# üìå UNIQUE + SHUFFLE\n",
        "# -----------------------------------------------------------\n",
        "print(\"\\nüîç Removing duplicate paths...\")\n",
        "all_nonret = list(set(all_nonret))\n",
        "\n",
        "# üî• FULL SHUFFLE AGAIN FOR MAXIMUM VARIETY\n",
        "random.shuffle(all_nonret)\n",
        "\n",
        "print(f\"üìä UNIQUE non-retinal images collected: {len(all_nonret)}\")\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# 5) SAMPLE EXACT TARGET (WITH SHUFFLING)\n",
        "# ================================================================\n",
        "print(\"\\nüìå STEP 6: Sampling EXACT 30k Train + 15k Val...\")\n",
        "\n",
        "nonret_train = all_nonret[:NONRET_TRAIN_TARGET]\n",
        "nonret_val   = all_nonret[NONRET_TRAIN_TARGET:NONRET_TRAIN_TARGET + NONRET_VAL_TARGET]\n",
        "\n",
        "# üî• SHUFFLE TRAIN & VAL AGAIN FOR DIVERSITY\n",
        "random.shuffle(nonret_train)\n",
        "random.shuffle(nonret_val)\n",
        "\n",
        "print(f\"‚úî Non-Retinal Train Selected: {len(nonret_train)}\")\n",
        "print(f\"‚úî Non-Retinal Val  Selected: {len(nonret_val)}\")\n",
        "\n",
        "# ================================================================\n",
        "# COPY NON-RETINAL IMAGES\n",
        "# ================================================================\n",
        "print(\"\\nüì§ Copying NON-RETINAL TRAIN images...\")\n",
        "for img in tqdm(nonret_train, desc=\"Copying Non-Retinal Train\"):\n",
        "    shutil.copy(img, paths[\"train_nonret\"])\n",
        "\n",
        "print(\"\\nüì§ Copying NON-RETINAL VAL images...\")\n",
        "for img in tqdm(nonret_val, desc=\"Copying Non-Retinal Val\"):\n",
        "    shutil.copy(img, paths[\"val_nonret\"])\n",
        "\n",
        "print(\"\\nüî• Cleaning nonret_raw to save disk...\")\n",
        "shutil.rmtree(nonret_raw)\n",
        "\n",
        "# ================================================================\n",
        "# 6) ZIP FINAL DATASET\n",
        "# ================================================================\n",
        "print(\"\\nüìå STEP 7: Creating ZIP file (Memory-Safe)...\")\n",
        "shutil.make_archive(\"/content/retina_nonretina_dataset_balanced2\", \"zip\", base_dir)\n",
        "\n",
        "final_zip = \"/content/retina_nonretina_dataset_balanced2.zip\"\n",
        "print(\"\\nüéâ DONE! ZIP FILE CREATED SUCCESSFULLY!\")\n",
        "print(\"üì¶ Download ZIP at:\", final_zip)\n",
        "\n",
        "# ================================================================\n",
        "# 7) UPLOAD TO GOOGLE DRIVE\n",
        "# ================================================================\n",
        "print(\"\\nüìå STEP 8: Uploading ZIP to Google Drive...\")\n",
        "\n",
        "\n",
        "\n",
        "drive_target = \"/content/drive/MyDrive/retina_nonretina_dataset_balanced2.zip\"\n",
        "shutil.copy(final_zip, drive_target)\n",
        "\n",
        "print(\"\\n‚úî File uploaded to Drive at:\", drive_target)\n",
        "print(\"\\nüöÄ DATASET GENERATION PIPELINE COMPLETE!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBl0nfP2pxhx",
        "outputId": "a73ef93e-e510-441f-a86a-b5655b5f9187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üöÄ STARTING BALANCED DATASET GENERATION PIPELINE...\n",
            "\n",
            "üìå STEP 1: Installing KaggleHub & Downloading Retinal Dataset\n",
            "üì• Downloading retinal dataset via KaggleHub...\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/ascanipek/eyepacs-aptos-messidor-diabetic-retinopathy?dataset_version_number=4...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20.5G/20.5G [03:40<00:00, 99.5MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úî Retina dataset downloaded at: /root/.cache/kagglehub/datasets/ascanipek/eyepacs-aptos-messidor-diabetic-retinopathy/versions/4\n",
            "\n",
            "üìå STEP 2: Collecting retinal images...\n",
            "üîç Found 236170 raw retinal images.\n",
            "üìä Retinal Train Count: 20000\n",
            "üìä Retinal Val Count:   8000\n",
            "\n",
            "üìå STEP 3: Creating final dataset folder structure...\n",
            "‚úî Final folders created!\n",
            "\n",
            "üìå STEP 4: Copying retinal TRAIN images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying Retinal Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20000/20000 [01:03<00:00, 315.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìå Copying retinal VAL images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying Retinal Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8000/8000 [00:23<00:00, 340.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìå STEP 5: Collecting Non-Retinal Images from CIFAR & COCO...\n",
            "\n",
            "üì• Loading CIFAR-10 and CIFAR-100...\n",
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n",
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "\u001b[1m169001437/169001437\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n",
            "üñº Saving cifar10 images...\n",
            "üñº Saving cifar100 images...\n",
            "\n",
            "üì• Downloading COCO 2017 train subset (~40,000 images)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading COCO:   0%|          | 0.00/19.3G [05:13<?, ?B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Extracting 40,000 COCO images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting COCO: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40000/40000 [02:42<00:00, 246.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Removing duplicate paths...\n",
            "üìä UNIQUE non-retinal images collected: 140000\n",
            "\n",
            "üìå STEP 6: Sampling EXACT 30k Train + 15k Val...\n",
            "‚úî Non-Retinal Train Selected: 20000\n",
            "‚úî Non-Retinal Val  Selected: 8000\n",
            "\n",
            "üì§ Copying NON-RETINAL TRAIN images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying Non-Retinal Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20000/20000 [00:40<00:00, 497.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üì§ Copying NON-RETINAL VAL images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying Non-Retinal Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8000/8000 [00:13<00:00, 603.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üî• Cleaning nonret_raw to save disk...\n",
            "\n",
            "üìå STEP 7: Creating ZIP file (Memory-Safe)...\n",
            "\n",
            "üéâ DONE! ZIP FILE CREATED SUCCESSFULLY!\n",
            "üì¶ Download ZIP at: /content/retina_nonretina_dataset_balanced2.zip\n",
            "\n",
            "üìå STEP 8: Uploading ZIP to Google Drive...\n",
            "\n",
            "‚úî File uploaded to Drive at: /content/drive/MyDrive/retina_nonretina_dataset_balanced2.zip\n",
            "\n",
            "üöÄ DATASET GENERATION PIPELINE COMPLETE!\n"
          ]
        }
      ]
    }
  ]
}